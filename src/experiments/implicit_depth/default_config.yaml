# general options
trainer_name: 
exp_type: 
base_log_dir: 
log_name: 
custom_postfix: ''
checkpoint_path: 
resume: 
gpu_id: 
vis_gpu: 
seed: 
debug: False
mask_type: all

# data setting
dataset:
  type: 
  cleargrasp_root_dir:
  omniverse_root_dir:
  use_data_augmentation: False
  img_width: 320
  img_height: 240
  split_ratio: 0.9
  max_depth: 4
  omni_corrupt_all: True
  corrupt_table: True
  depth_aug: False
  corrupt_all_pix: False
  # ellipse dropout
  ellipse_dropout_mean: 20
  ellipse_gamma_shape: 10.0
  ellipse_gamma_scale: 1.0
  # Multiplicative noise
  gamma_shape: 1000.
  gamma_scale: 0.001
  # Additive noise
  gaussian_scale: 0.005 # 5mm standard dev
  gp_rescale_factor: 4

# model setting
model:
  # rgb
  rgb_model_type: 
  rgb_embedding_type: 
  rgb_in: 3
  rgb_out: 
  roi_inp_bbox: 8
  roi_out_bbox: 2
  # pnet
  pnet_model_type: 
  pnet_in: 6
  pnet_out: 
  pnet_gf: 32
  pnet_pos_type: rel
  # positional encoding
  pos_encode: True
  intersect_pos_type: abs
  multires: 8
  multires_views: 4
  # decoder
  offdec_type: IEF
  n_iter: 2
  probdec_type: IMNET
  imnet_gf: 64
  scatter_type: Maxpool
  use_sigmoid: False
  maxpool_label_epo: 6


# grid setting
grid:
  res: 
  miss_sample_num: 20000
  valid_sample_num: 10000
  offset_range: [0.,1.]

# training setting
training:
  batch_size: 32
  valid_batch_size: 1
  nepochs: 30
  nepoch_decay: 30
  decay_gamma: 0.1
  nepoch_ckpt: 1
  log_interval: 5
  train_vis_iter: 4
  val_vis_iter: 8
  test_vis_iter: 40
  lr: 0.001
  num_workers: 4
  pin_memory: False
  do_valid: True
  valid_start_epo: 0
  optimizer_name: Adam
  scheduler_name: StepLR

loss:
  hard_neg: False
  hard_neg_ratio: 
  pos_loss_type: single
  pos_w: 100.0
  prob_loss_type: ray
  prob_w: 0.5
  surf_norm_w: 10.0
  surf_norm_epo: 0
  smooth_w: 0
  smooth_epo: 0

# distributed setting, only used it when multiprocessing-distributed set to True
dist:
  ddp: False
  dist_url: 
  dist_backend: 
  # nodes number
  nodes_num:
  # rank of current node
  node_rank: 
  # GPUs/Process number per node
  ngpus_per_node: 
  # totol GPU number. eequal to nodes_num * ngpus_per_node. handled by create trainer
  world_size: 
  # gpu id among all nodes all processes, handled by create trainer
  global_gpu_id: